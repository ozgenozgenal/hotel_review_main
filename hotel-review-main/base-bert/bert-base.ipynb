{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Importations\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 'ISO-8859-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     hotel_name hotel_city  review_date  \\\n",
      "0  china_beijing_ascott_beijing    beijing  Aug 17 2009   \n",
      "1  china_beijing_ascott_beijing    beijing  Mar 25 2009   \n",
      "2  china_beijing_ascott_beijing    beijing  Nov 18 2008   \n",
      "3  china_beijing_ascott_beijing    beijing  Sep 20 2008   \n",
      "4  china_beijing_ascott_beijing    beijing   Nov 1 2007   \n",
      "\n",
      "                                        hotel_review  \n",
      "0  don't rely on it if you have any mission-criti...  \n",
      "1  Excellent hotel for a family\\tThis hotel is pe...  \n",
      "2  Choice for Western Visitors\\tI stayed at The A...  \n",
      "3  Really Good Alternative Accomodation in Beijin...  \n",
      "4  Didn't want to leave!\\tWe rented a two bedroom...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_hotel_reviews(data_dir):\n",
    "    \n",
    "    date_pattern = re.compile(r'(\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{1,2}\\s\\d{4})\\t(.*)')\n",
    "    data_frames = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        city = os.path.basename(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            hotel_name = os.path.splitext(file)[0]  # Assuming the file has no extension\n",
    "\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            reviews = date_pattern.findall(content)\n",
    "            reviews_data = [{'hotel_name': hotel_name, 'hotel_city': city, 'review_date': date, 'hotel_review': review.strip()} for date, _, review in reviews]\n",
    "            if reviews_data:\n",
    "                data_frames.append(pd.DataFrame(reviews_data))\n",
    "\n",
    "    if data_frames:\n",
    "        review_df = pd.concat(data_frames, ignore_index=True)\n",
    "    else:\n",
    "        review_df = pd.DataFrame(columns=['hotel_name', 'hotel_city', 'review_date', 'hotel_review'])\n",
    "    return review_df\n",
    "\n",
    "data_dir = 'data'\n",
    "review_df = parse_hotel_reviews(data_dir)\n",
    "print(review_df.head())\n",
    "review_df.to_csv('csv/derlenmis_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('csv/derlenmis_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df.dropna(subset=['hotel_review'])\n",
    "review_df['sentences'] = review_df['hotel_review'].apply(sent_tokenize)\n",
    "review_df.to_csv('csv/processed_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('csv/processed_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = ['cleanliness', 'room', 'service', 'location', 'value', 'safety', 'comfort', 'transportation', 'noise']\n",
    "aspect_keywords  = {\n",
    "    'cleanliness': {\n",
    "        'positive': ['clean', 'very clean', 'perfectly clean', 'well maintained', 'spotless', 'tidy', 'very tidy', 'smells good'],\n",
    "        'negative': ['not clean', 'dirty', 'very dirty', 'stain', 'poorly maintained', 'smells bad', 'stink', 'stunk']\n",
    "    },\n",
    "    'room': {\n",
    "        'positive': ['spacious room', 'large room', 'comfortable bed', 'beautiful room', 'comfortable', 'big suite', 'huge room'],\n",
    "        'negative': ['small room', 'tiny room', 'uncomfortable bed', 'outdated room', 'noisy room', 'uncomfortable']\n",
    "    },\n",
    "    'service': {\n",
    "        'positive': ['friendly', 'good staff', 'good service', 'excellent staff', 'excellent service', 'helpful staff', 'nice service', 'excellent service', 'good assistance'],\n",
    "        'negative': ['rude staff', 'rude', 'poor service', 'unhelpful', 'slow service']\n",
    "    },\n",
    "    'location': {\n",
    "        'positive': ['great location', 'perfect location', 'convenient location', 'ideal location', 'central location', 'good location'],\n",
    "        'negative': ['bad location', 'worst location', 'inconvenient location', 'unsafe area', 'unsafe', 'far away', 'far']\n",
    "    },\n",
    "    'value': {\n",
    "        'positive': ['good value', 'worth', 'price performance', 'great price', 'affordable', 'worth every penny', 'reasonable price', 'not expensive', 'cheap'],\n",
    "        'negative': ['overpriced', 'not worth the money', 'too expensive', 'poor value', 'rip off']\n",
    "    },\n",
    "    'safety': {\n",
    "        'positive': ['secure parking', 'high security', 'safe', 'secure', 'safe and sound'],\n",
    "        'negative': ['unsafe', 'dangerous', 'poor security', 'risky', 'not secure', 'not safe']\n",
    "    },\n",
    "    'comfort': {\n",
    "        'positive': ['very comfortable', 'comfortable', 'relaxing', 'peaceful', 'quiet', 'hot shower'],\n",
    "        'negative': ['uncomfortable', 'very uncomfortable', 'hard bed', 'uncomfortable chairs', 'noise', 'noisy', 'poor insulation']\n",
    "    },\n",
    "    'transportation': {\n",
    "        'positive': ['close to subway', 'shuttle', 'near the airport', 'good transport links', 'ample parking', 'close to transportation', 'close to bus', 'close to station', 'close to metro', 'close to airport', 'near the bus', 'near the station', 'near the metro', 'walking distance', 'taxi'],\n",
    "        'negative': ['far from subway', 'no shuttle', 'far from airport', 'poor transportatiton', 'limited parking', 'far away', 'far']\n",
    "    },\n",
    "    'noise': {\n",
    "        'positive': ['quiet room', 'soundproof', 'no noise', 'peaceful', 'quiet', 'silent'],\n",
    "        'negative': ['noisy room', 'loud neighbors', 'traffic noise', 'thin walls', 'can hear everything', 'high volume', 'noisy', 'noise']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_sentiment(sentence):\n",
    "    aspect_sentiment = {aspect: 0 for aspect in aspects}  # Initialize all aspects with 0\n",
    "    for aspect, keywords in aspect_keywords.items():\n",
    "        for sentiment, keys in keywords.items():\n",
    "            if any(key in sentence.lower() for key in keys):\n",
    "                # Assuming simple positive/negative, where positive = 1, negative = -1\n",
    "                aspect_sentiment[aspect] = 1 if sentiment == 'positive' else -1\n",
    "    return aspect_sentiment\n",
    "\n",
    "review_df['aspect_sentiment'] = review_df['sentences'].apply(lambda sentences: [get_aspect_sentiment(sentence) for sentence in sentences])\n",
    "review_df.to_csv('csv/aspect_sentimented_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('csv/aspect_sentimented_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'processed_df' is loaded and contains 'hotel_review'\n",
    "flat_data = []\n",
    "\n",
    "for index, row in review_df.iterrows():\n",
    "    sentences = sent_tokenize(row['hotel_review'])\n",
    "    for sentence in sentences:\n",
    "        sentiment_scores = get_aspect_sentiment(sentence)\n",
    "        flat_data.append({\n",
    "            'hotel_name': row['hotel_name'],\n",
    "            'hotel_city': row['hotel_city'],\n",
    "            'review_date': row['review_date'],\n",
    "            'sentence': sentence,\n",
    "            **sentiment_scores  # Expand the sentiment scores into separate columns\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame for training\n",
    "training_df = pd.DataFrame(flat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>hotel_city</th>\n",
       "      <th>review_date</th>\n",
       "      <th>sentence</th>\n",
       "      <th>cleanliness</th>\n",
       "      <th>room</th>\n",
       "      <th>service</th>\n",
       "      <th>location</th>\n",
       "      <th>value</th>\n",
       "      <th>safety</th>\n",
       "      <th>comfort</th>\n",
       "      <th>transportation</th>\n",
       "      <th>noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china_beijing_ascott_beijing</td>\n",
       "      <td>beijing</td>\n",
       "      <td>Aug 17 2009</td>\n",
       "      <td>don't rely on it if you have any mission-criti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>china_beijing_ascott_beijing</td>\n",
       "      <td>beijing</td>\n",
       "      <td>Aug 17 2009</td>\n",
       "      <td>I was not able to connect to my US stock/futur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>china_beijing_ascott_beijing</td>\n",
       "      <td>beijing</td>\n",
       "      <td>Aug 17 2009</td>\n",
       "      <td>couldn't login to my airline to do an internet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>china_beijing_ascott_beijing</td>\n",
       "      <td>beijing</td>\n",
       "      <td>Aug 17 2009</td>\n",
       "      <td>Other websites such as yahoo, msn messenger se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china_beijing_ascott_beijing</td>\n",
       "      <td>beijing</td>\n",
       "      <td>Aug 17 2009</td>\n",
       "      <td>the front desk tried it as well and they have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60393</th>\n",
       "      <td>usa_illinois_chicago_w_chicago_lakeshore</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Sep 3 2002</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60394</th>\n",
       "      <td>usa_illinois_chicago_w_chicago_lakeshore</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Jul 29 2002</td>\n",
       "      <td>Great view of the lake and convenient location!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60395</th>\n",
       "      <td>usa_illinois_chicago_w_chicago_lakeshore</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Jul 22 2002</td>\n",
       "      <td>Overpriced</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60396</th>\n",
       "      <td>usa_illinois_chicago_w_chicago_lakeshore</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Jul 8 2002</td>\n",
       "      <td>Simply --wild--not your regular Holiday Inn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60397</th>\n",
       "      <td>usa_illinois_chicago_w_chicago_lakeshore</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Mar 13 2009</td>\n",
       "      <td>Gut gelegenes Business Hotel mit Detailschwächen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60398 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     hotel_name hotel_city  review_date  \\\n",
       "0                  china_beijing_ascott_beijing    beijing  Aug 17 2009   \n",
       "1                  china_beijing_ascott_beijing    beijing  Aug 17 2009   \n",
       "2                  china_beijing_ascott_beijing    beijing  Aug 17 2009   \n",
       "3                  china_beijing_ascott_beijing    beijing  Aug 17 2009   \n",
       "4                  china_beijing_ascott_beijing    beijing  Aug 17 2009   \n",
       "...                                         ...        ...          ...   \n",
       "60393  usa_illinois_chicago_w_chicago_lakeshore    chicago   Sep 3 2002   \n",
       "60394  usa_illinois_chicago_w_chicago_lakeshore    chicago  Jul 29 2002   \n",
       "60395  usa_illinois_chicago_w_chicago_lakeshore    chicago  Jul 22 2002   \n",
       "60396  usa_illinois_chicago_w_chicago_lakeshore    chicago   Jul 8 2002   \n",
       "60397  usa_illinois_chicago_w_chicago_lakeshore    chicago  Mar 13 2009   \n",
       "\n",
       "                                                sentence  cleanliness  room  \\\n",
       "0      don't rely on it if you have any mission-criti...            0     0   \n",
       "1      I was not able to connect to my US stock/futur...            0     0   \n",
       "2      couldn't login to my airline to do an internet...            0     0   \n",
       "3      Other websites such as yahoo, msn messenger se...            0     0   \n",
       "4      the front desk tried it as well and they have ...            0     0   \n",
       "...                                                  ...          ...   ...   \n",
       "60393                                                  !            0     0   \n",
       "60394    Great view of the lake and convenient location!            0     0   \n",
       "60395                                         Overpriced            0     0   \n",
       "60396        Simply --wild--not your regular Holiday Inn            0     0   \n",
       "60397   Gut gelegenes Business Hotel mit Detailschwächen            0     0   \n",
       "\n",
       "       service  location  value  safety  comfort  transportation  noise  \n",
       "0            0         0      0       0        0               0      0  \n",
       "1            0         0      0       0        0               0      0  \n",
       "2            0         0      0       0        0               0      0  \n",
       "3            0         0      0       0        0               0      0  \n",
       "4            0         0      0       0        0               0      0  \n",
       "...        ...       ...    ...     ...      ...             ...    ...  \n",
       "60393        0         0      0       0        0               0      0  \n",
       "60394        0         1      0       0        0               0      0  \n",
       "60395        0         0     -1       0        0               0      0  \n",
       "60396        0         0      0       0        0               0      0  \n",
       "60397        0         0      0       0        0               0      0  \n",
       "\n",
       "[60398 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.to_csv('csv/training_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7550 [08:38<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "training_df.to_excel('csv/training_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\durud\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "training_df['input_ids'] = training_df['sentence'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class AspectSentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {'input_ids': torch.tensor(self.encodings[idx])}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare the dataset\n",
    "labels = training_df[['cleanliness', 'room', 'service', 'location', 'value', 'safety', 'comfort', 'transportation', 'noise']].values\n",
    "dataset = AspectSentimentDataset(training_df['input_ids'].tolist(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\durud\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=9)  # We have 9 aspects\n",
    "\n",
    "class HotelReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "encodings = tokenizer(training_df['sentence'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "labels = training_df[['cleanliness', 'room', 'service', 'location', 'value', 'safety', 'comfort', 'transportation', 'noise']].values\n",
    "dataset = HotelReviewDataset(encodings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "num_train_epochs=1\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
